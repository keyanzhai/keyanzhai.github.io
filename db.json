{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/Academia/source/attaches/CV_KeyanZhai_new.pdf","path":"attaches/CV_KeyanZhai_new.pdf","modified":1,"renderable":1},{"_id":"themes/Academia/source/css/index.styl","path":"css/index.styl","modified":1,"renderable":1},{"_id":"themes/Academia/source/css/user.styl","path":"css/user.styl","modified":1,"renderable":1},{"_id":"themes/Academia/source/img/KZ-AI.png","path":"img/KZ-AI.png","modified":1,"renderable":1},{"_id":"themes/Academia/source/img/KZ.JPG","path":"img/KZ.JPG","modified":1,"renderable":1},{"_id":"themes/Academia/source/img/profile.png","path":"img/profile.png","modified":1,"renderable":1},{"_id":"themes/Academia/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"source/Projects/Elevator/resources/Button_Detect.JPG","path":"Projects/Elevator/resources/Button_Detect.JPG","modified":1,"renderable":0},{"_id":"source/Projects/Elevator/resources/Button_Push.jpg","path":"Projects/Elevator/resources/Button_Push.jpg","modified":1,"renderable":0},{"_id":"source/Projects/Elevator/resources/Elevator_Robot.JPG","path":"Projects/Elevator/resources/Elevator_Robot.JPG","modified":1,"renderable":0},{"_id":"source/Projects/Elevator/resources/Navi.png","path":"Projects/Elevator/resources/Navi.png","modified":1,"renderable":0},{"_id":"source/Projects/Elevator/resources/elevator.mp4","path":"Projects/Elevator/resources/elevator.mp4","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/Comparison_Body_V_Pos_new.jpg","path":"Publications/Slimdog/resources/Comparison_Body_V_Pos_new.jpg","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/Comparison_Joint_Angles.jpg","path":"Publications/Slimdog/resources/Comparison_Joint_Angles.jpg","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/No_P1_R1_Kinovea3.mp4","path":"Publications/Slimdog/resources/No_P1_R1_Kinovea3.mp4","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/Min_P1_R1_Kinovea3.mp4","path":"Publications/Slimdog/resources/Min_P1_R1_Kinovea3.mp4","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/Comparison_min_red_no.png","path":"Publications/Slimdog/resources/Comparison_min_red_no.png","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/Red_P1_R3_Kinovea3.mp4","path":"Publications/Slimdog/resources/Red_P1_R3_Kinovea3.mp4","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/Slimdog1.JPG","path":"Publications/Slimdog/resources/Slimdog1.JPG","modified":1,"renderable":0},{"_id":"source/Publications/Slimdog/resources/failures.mp4","path":"Publications/Slimdog/resources/failures.mp4","modified":1,"renderable":0}],"Cache":[{"_id":"themes/Academia/README.md","hash":"74f4125078ef09e1753cc0da318378507163b707","modified":1688782866563},{"_id":"themes/Academia/LICENSE","hash":"f5c9b9f5d232b115c8846ad6ce1218998bcaec80","modified":1688782866563},{"_id":"themes/Academia/_config.yml","hash":"c82604b863571a1f897aee88bfa5a1c429d9a71c","modified":1691983027512},{"_id":"themes/Academia/layout/index.pug","hash":"d7f5491090f6fba6b00efea99fbb2ab6191c8052","modified":1688782866564},{"_id":"themes/Academia/scripts/datafile.js","hash":"47ee0429c087dc0472b8ba841230139941db4003","modified":1688782866564},{"_id":"themes/Academia/layout/page.pug","hash":"2c154231297e2e36102e6031dadae65b5c1a564c","modified":1688782866564},{"_id":"themes/Academia/layout/includes/footer.pug","hash":"37b68e83b99b31ef8d161c53f542eb80b0d07bb2","modified":1688783350951},{"_id":"themes/Academia/layout/includes/layout.pug","hash":"8d2fc30d5d8961a11e25e691abfda920a377df1b","modified":1688782866564},{"_id":"themes/Academia/layout/includes/header.pug","hash":"564c213f9eea287f9d210cbe2251c815ba9562da","modified":1688782866564},{"_id":"themes/Academia/layout/includes/mobile-nav.pug","hash":"3cf266fc93890fe8aabf75eee8d30a437f016539","modified":1688782866564},{"_id":"themes/Academia/layout/includes/side-card.pug","hash":"84ba9f57114fd847afb41d3bb5e5cbd51d2efa5b","modified":1688782866564},{"_id":"themes/Academia/source/css/user.styl","hash":"74c58ccfda6c7ee8d10c91261afaf3ced19fe9a9","modified":1688782866565},{"_id":"themes/Academia/source/css/index.styl","hash":"978ac6e8ca1fea0ad7bd8347e44d67e5a48e21e8","modified":1688941079618},{"_id":"themes/Academia/source/img/profile.png","hash":"5358b67cb793572aac422e23ad7db401e1c7ac2b","modified":1688782866565},{"_id":"themes/Academia/source/js/main.js","hash":"3602dddaad6d61d18a521fcddd500921959237b2","modified":1688782866566},{"_id":"themes/Academia/source/css/_highlight/highlight.styl","hash":"618e8fea76b57dd6e79fa0fb4557ea1907bf7d1b","modified":1688782866565},{"_id":"themes/Academia/source/attaches/CV_KeyanZhai_new.pdf","hash":"7b9e96c986f12fe1bffa472f836148bcc0a37add","modified":1700588687940},{"_id":"source/Projects/index.md","hash":"e57e93f884711cdf2eb0e265794fe62e448d9bcf","modified":1700588925186},{"_id":"source/_posts/Introduction.md","hash":"33afbbe4e18925e9f1b346af9cfa02666de09701","modified":1700588925325},{"_id":"source/Publications/index.md","hash":"6447090987746d4fda514f6f679c938c7698bfc5","modified":1700588925324},{"_id":"source/Publications/Slimdog/index.md","hash":"b3b84c2e07b5c64cc7ca6677a1d6d43ca33a15e3","modified":1700588925186},{"_id":"source/Projects/Elevator/resources/Button_Detect.JPG","hash":"98ea442537e42fab073f4bdddfead2c160399864","modified":1700588925123},{"_id":"source/Projects/Elevator/index.md","hash":"0025bcb532ad950c14d1b0dd5d5b076cbce2e72b","modified":1700588925123},{"_id":"source/Projects/Elevator/resources/Button_Push.jpg","hash":"90f17e437130d5874826e4373701e82ea6faf12e","modified":1700588925124},{"_id":"themes/Academia/source/img/KZ.JPG","hash":"7aa46c76e76e9ab5a29fe11db290e24fa93788e4","modified":1688783142763},{"_id":"source/Publications/Slimdog/resources/Comparison_min_red_no.png","hash":"81686e4ada454a9ef36b70481cf714adde00ab33","modified":1700588925197},{"_id":"source/Publications/Slimdog/resources/Comparison_Joint_Angles.jpg","hash":"70d394a4a6f9c9365d817052ffae59ccf43250dc","modified":1700588925196},{"_id":"source/Publications/Slimdog/resources/Comparison_Body_V_Pos_new.jpg","hash":"74ea45a8b1ec43c5453392161a70eefb55dc5ce8","modified":1700588925192},{"_id":"themes/Academia/source/img/KZ-AI.png","hash":"076b1d3ab3b7819aac6c4b9d6cde3c2f7f6e6db6","modified":1688783098043},{"_id":"source/Publications/Slimdog/resources/Slimdog1.JPG","hash":"e2a82857c6249d6e34b43d7916371d2cccb853d5","modified":1700588925228},{"_id":"source/Publications/Slimdog/resources/Min_P1_R1_Kinovea3.mp4","hash":"b02ae787d2e2e9c7bb1a6a7c8f7e7937e9225554","modified":1700588925202},{"_id":"source/Publications/Slimdog/resources/Red_P1_R3_Kinovea3.mp4","hash":"768b9f2aedd7dc77d886e6b06a2856fca946e7eb","modified":1700588925219},{"_id":"source/Projects/Elevator/resources/Navi.png","hash":"a7d3fbd5a8803166fd04dee72b6bf2aeda85c183","modified":1700588925144},{"_id":"source/Publications/Slimdog/resources/No_P1_R1_Kinovea3.mp4","hash":"4b165e24d2b8d431c25015ff0cf1d9fd5fe65397","modified":1700588925214},{"_id":"source/Projects/Elevator/resources/Elevator_Robot.JPG","hash":"2e16662d8e1caadd655f35575d97c3e91d10afb9","modified":1700588925133},{"_id":"source/Projects/Elevator/resources/elevator.mp4","hash":"5cfd33b559b42685209e06b52a6b59e29def4d2c","modified":1700588925185},{"_id":"source/Publications/Slimdog/resources/failures.mp4","hash":"1852c44e4096e98cf1249566e22c7988e549d28e","modified":1700588925324},{"_id":"public/Projects/index.html","hash":"a8fc74a1c3b9fd596a0092f88c2f63bcae4de95d","modified":1700588958322},{"_id":"public/Publications/index.html","hash":"b8dab30ea9e4cc1db52c11f8debd6e351fb68c2f","modified":1700588958322},{"_id":"public/Publications/Slimdog/index.html","hash":"93ebe5bc08f565d87f51fd608138f23fce8a8215","modified":1700588958322},{"_id":"public/Projects/Elevator/index.html","hash":"069211ac6020b631bcdfb24dcb25927168a400b5","modified":1700588958322},{"_id":"public/2023/07/02/Introduction/index.html","hash":"a0dec2f8ac4edc687a2fcb733e6b6a950fd69a99","modified":1700588958322},{"_id":"public/archives/index.html","hash":"9535a9b02709163ae45eb2e2ca933ae6d77ca31e","modified":1700588958322},{"_id":"public/archives/2023/index.html","hash":"9535a9b02709163ae45eb2e2ca933ae6d77ca31e","modified":1700588958322},{"_id":"public/archives/2023/07/index.html","hash":"9535a9b02709163ae45eb2e2ca933ae6d77ca31e","modified":1700588958322},{"_id":"public/index.html","hash":"9535a9b02709163ae45eb2e2ca933ae6d77ca31e","modified":1700588958322},{"_id":"public/Projects/Elevator/resources/Button_Detect.JPG","hash":"98ea442537e42fab073f4bdddfead2c160399864","modified":1700588958322},{"_id":"public/img/profile.png","hash":"5358b67cb793572aac422e23ad7db401e1c7ac2b","modified":1700588958322},{"_id":"public/attaches/CV_KeyanZhai_new.pdf","hash":"7b9e96c986f12fe1bffa472f836148bcc0a37add","modified":1700588958322},{"_id":"public/css/user.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1700588958322},{"_id":"public/js/main.js","hash":"faf9b5cfaecf81fd2027620279e083ddab481c3b","modified":1700588958322},{"_id":"public/css/index.css","hash":"5e9a570ed161f57620fa35478cbe152390e4fde5","modified":1700588958322},{"_id":"public/Projects/Elevator/resources/Button_Push.jpg","hash":"90f17e437130d5874826e4373701e82ea6faf12e","modified":1700588958322},{"_id":"public/img/KZ.JPG","hash":"7aa46c76e76e9ab5a29fe11db290e24fa93788e4","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/Comparison_min_red_no.png","hash":"81686e4ada454a9ef36b70481cf714adde00ab33","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/Comparison_Joint_Angles.jpg","hash":"70d394a4a6f9c9365d817052ffae59ccf43250dc","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/Comparison_Body_V_Pos_new.jpg","hash":"74ea45a8b1ec43c5453392161a70eefb55dc5ce8","modified":1700588958322},{"_id":"public/img/KZ-AI.png","hash":"076b1d3ab3b7819aac6c4b9d6cde3c2f7f6e6db6","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/Slimdog1.JPG","hash":"e2a82857c6249d6e34b43d7916371d2cccb853d5","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/Min_P1_R1_Kinovea3.mp4","hash":"b02ae787d2e2e9c7bb1a6a7c8f7e7937e9225554","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/Red_P1_R3_Kinovea3.mp4","hash":"768b9f2aedd7dc77d886e6b06a2856fca946e7eb","modified":1700588958322},{"_id":"public/Projects/Elevator/resources/Navi.png","hash":"a7d3fbd5a8803166fd04dee72b6bf2aeda85c183","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/No_P1_R1_Kinovea3.mp4","hash":"4b165e24d2b8d431c25015ff0cf1d9fd5fe65397","modified":1700588958322},{"_id":"public/Projects/Elevator/resources/Elevator_Robot.JPG","hash":"2e16662d8e1caadd655f35575d97c3e91d10afb9","modified":1700588958322},{"_id":"public/Projects/Elevator/resources/elevator.mp4","hash":"5cfd33b559b42685209e06b52a6b59e29def4d2c","modified":1700588958322},{"_id":"public/Publications/Slimdog/resources/failures.mp4","hash":"1852c44e4096e98cf1249566e22c7988e549d28e","modified":1700588958322}],"Category":[],"Data":[],"Page":[{"title":"Projects","date":"2023-07-02T06:42:34.000Z","_content":"\n### [Task Management Web App](https://flaskcrudapptutorial-kz-67fea61b1f9b.herokuapp.com/)\n\nA simple CRUD Web Application with Python, Flask and SQLite. Deployed on Heroku.\n\nReference: <https://www.youtube.com/watch?v=Z1RJmh_OqeA>\n\n\n### [Autonomous Elevator Robot](/Projects/Elevator/)\n\nIn this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.","source":"Projects/index.md","raw":"---\ntitle: Projects\ndate: 2023-07-02 02:42:34\n---\n\n### [Task Management Web App](https://flaskcrudapptutorial-kz-67fea61b1f9b.herokuapp.com/)\n\nA simple CRUD Web Application with Python, Flask and SQLite. Deployed on Heroku.\n\nReference: <https://www.youtube.com/watch?v=Z1RJmh_OqeA>\n\n\n### [Autonomous Elevator Robot](/Projects/Elevator/)\n\nIn this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.","updated":"2023-11-21T17:48:45.186Z","path":"Projects/index.html","comments":1,"layout":"page","_id":"clp8mraf50000b7k8gk6rh6vz","content":"<h3 id=\"Task-Management-Web-App\"><a href=\"#Task-Management-Web-App\" class=\"headerlink\" title=\"Task Management Web App\"></a><a href=\"https://flaskcrudapptutorial-kz-67fea61b1f9b.herokuapp.com/\">Task Management Web App</a></h3><p>A simple CRUD Web Application with Python, Flask and SQLite. Deployed on Heroku.</p>\n<p>Reference: <a href=\"https://www.youtube.com/watch?v=Z1RJmh_OqeA\">https://www.youtube.com/watch?v=Z1RJmh_OqeA</a></p>\n<h3 id=\"Autonomous-Elevator-Robot\"><a href=\"#Autonomous-Elevator-Robot\" class=\"headerlink\" title=\"Autonomous Elevator Robot\"></a><a href=\"/Projects/Elevator/\">Autonomous Elevator Robot</a></h3><p>In this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Task-Management-Web-App\"><a href=\"#Task-Management-Web-App\" class=\"headerlink\" title=\"Task Management Web App\"></a><a href=\"https://flaskcrudapptutorial-kz-67fea61b1f9b.herokuapp.com/\">Task Management Web App</a></h3><p>A simple CRUD Web Application with Python, Flask and SQLite. Deployed on Heroku.</p>\n<p>Reference: <a href=\"https://www.youtube.com/watch?v=Z1RJmh_OqeA\">https://www.youtube.com/watch?v=Z1RJmh_OqeA</a></p>\n<h3 id=\"Autonomous-Elevator-Robot\"><a href=\"#Autonomous-Elevator-Robot\" class=\"headerlink\" title=\"Autonomous Elevator Robot\"></a><a href=\"/Projects/Elevator/\">Autonomous Elevator Robot</a></h3><p>In this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.</p>\n"},{"title":"Publications","date":"2023-07-02T05:07:40.000Z","academia":true,"_content":"\n### [Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization](/Publications/Slimdog/)\n\n**Publication** [[**Springer**](https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28)][[**arXiv**](https://arxiv.org/abs/2101.09961)]: \nKeyan Zhai, Chu'an Li, and Andre Rosendo. \"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.\" 16th International Conference on Intelligent Autonomous Systems ([IAS-16](https://www.ias-16.com/)).\n\n**Abstract**\nDuring learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure/control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.","source":"Publications/index.md","raw":"---\ntitle: Publications\ndate: 2023-07-02 01:07:40\nacademia: true\n---\n\n### [Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization](/Publications/Slimdog/)\n\n**Publication** [[**Springer**](https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28)][[**arXiv**](https://arxiv.org/abs/2101.09961)]: \nKeyan Zhai, Chu'an Li, and Andre Rosendo. \"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.\" 16th International Conference on Intelligent Autonomous Systems ([IAS-16](https://www.ias-16.com/)).\n\n**Abstract**\nDuring learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure/control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.","updated":"2023-11-21T17:48:45.324Z","path":"Publications/index.html","comments":1,"layout":"page","_id":"clp8mraf80002b7k855vla2rd","content":"<h3 id=\"Scaffolded-Learning-of-In-place-Trotting-Gait-for-a-Quadruped-Robot-with-Bayesian-Optimization\"><a href=\"#Scaffolded-Learning-of-In-place-Trotting-Gait-for-a-Quadruped-Robot-with-Bayesian-Optimization\" class=\"headerlink\" title=\"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization\"></a><a href=\"/Publications/Slimdog/\">Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization</a></h3><p><strong>Publication</strong> [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28\"><strong>Springer</strong></a>][<a href=\"https://arxiv.org/abs/2101.09961\"><strong>arXiv</strong></a>]:<br>Keyan Zhai, Chu’an Li, and Andre Rosendo. “Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.” 16th International Conference on Intelligent Autonomous Systems (<a href=\"https://www.ias-16.com/\">IAS-16</a>).</p>\n<p><strong>Abstract</strong><br>During learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure&#x2F;control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Scaffolded-Learning-of-In-place-Trotting-Gait-for-a-Quadruped-Robot-with-Bayesian-Optimization\"><a href=\"#Scaffolded-Learning-of-In-place-Trotting-Gait-for-a-Quadruped-Robot-with-Bayesian-Optimization\" class=\"headerlink\" title=\"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization\"></a><a href=\"/Publications/Slimdog/\">Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization</a></h3><p><strong>Publication</strong> [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28\"><strong>Springer</strong></a>][<a href=\"https://arxiv.org/abs/2101.09961\"><strong>arXiv</strong></a>]:<br>Keyan Zhai, Chu’an Li, and Andre Rosendo. “Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.” 16th International Conference on Intelligent Autonomous Systems (<a href=\"https://www.ias-16.com/\">IAS-16</a>).</p>\n<p><strong>Abstract</strong><br>During learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure&#x2F;control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.</p>\n"},{"title":"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization","date":"2020-11-20T00:10:21.000Z","academia":true,"_content":"\n**Publication** [[**Springer**](https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28)][[**arXiv**](https://arxiv.org/abs/2101.09961)][[**code**](https://github.com/keyanzhai/Slimdog)][[**slides**](https://1drv.ms/p/s!AuXcCtfGaQlXg7QnW-4KvFFFTjsgGg?e=eXJ6RU)]: \nKeyan Zhai, Chu'an Li, and Andre Rosendo. \"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.\" 16th International Conference on Intelligent Autonomous Systems ([IAS-16](https://www.ias-16.com/)).\n\n**Abstract**\nDuring learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure/control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.\n\n## Motivation\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/failures.mp4\"></video></p>\n\n## Experiments\n\n<figure>\n  <img src=\"resources/Slimdog1.JPG\" alt=\"Slimdog\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Slimdog robot</figcaption>\n</figure>\n\n<figure>\n  <img src=\"resources/Comparison_min_red_no.png\" alt=\"Comparison of no support, minimum support and reducing support\" style=\"width:100%\">\n  <figcaption>Fig.2 - Comparison of no support, minimum support and reducing support</figcaption>\n</figure>\n\n## Kinovea Analysis\n\n### No support (No_P1, 1st run, fitness = 72.83)\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/No_P1_R1_Kinovea3.mp4\"></video></p>\n\n### Minimum support (Min_P1, 1st run, fitness = 72.43)\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Min_P1_R1_Kinovea3.mp4\"></video></p>\n\n### Reducing support (Red_P1, 3rd run, fitness = 75.92)\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Red_P1_R3_Kinovea3.mp4\"></video></p>\n\n\n## Results\n\n### Knee and joint angles\n\n<figure>\n  <img src=\"resources/Comparison_Joint_Angles.jpg\" alt=\"Knee and joint angles\" style=\"width:100%\">\n  <figcaption>Fig.3 - Knee and joint angles</figcaption>\n</figure>\n\n### Vertical translation of center of mass\n\n<figure>\n  <img src=\"resources/Comparison_Body_V_Pos_new.jpg\" alt=\"Vertical translation of center of mass\" style=\"width:100%\">\n  <figcaption>Fig.4 - Vertical translation of center of mass</figcaption>\n</figure>\n","source":"Publications/Slimdog/index.md","raw":"---\ntitle: Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization\ndate: 2020-11-19 19:10:21\nacademia: true\n---\n\n**Publication** [[**Springer**](https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28)][[**arXiv**](https://arxiv.org/abs/2101.09961)][[**code**](https://github.com/keyanzhai/Slimdog)][[**slides**](https://1drv.ms/p/s!AuXcCtfGaQlXg7QnW-4KvFFFTjsgGg?e=eXJ6RU)]: \nKeyan Zhai, Chu'an Li, and Andre Rosendo. \"Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.\" 16th International Conference on Intelligent Autonomous Systems ([IAS-16](https://www.ias-16.com/)).\n\n**Abstract**\nDuring learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure/control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.\n\n## Motivation\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/failures.mp4\"></video></p>\n\n## Experiments\n\n<figure>\n  <img src=\"resources/Slimdog1.JPG\" alt=\"Slimdog\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Slimdog robot</figcaption>\n</figure>\n\n<figure>\n  <img src=\"resources/Comparison_min_red_no.png\" alt=\"Comparison of no support, minimum support and reducing support\" style=\"width:100%\">\n  <figcaption>Fig.2 - Comparison of no support, minimum support and reducing support</figcaption>\n</figure>\n\n## Kinovea Analysis\n\n### No support (No_P1, 1st run, fitness = 72.83)\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/No_P1_R1_Kinovea3.mp4\"></video></p>\n\n### Minimum support (Min_P1, 1st run, fitness = 72.43)\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Min_P1_R1_Kinovea3.mp4\"></video></p>\n\n### Reducing support (Red_P1, 3rd run, fitness = 75.92)\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Red_P1_R3_Kinovea3.mp4\"></video></p>\n\n\n## Results\n\n### Knee and joint angles\n\n<figure>\n  <img src=\"resources/Comparison_Joint_Angles.jpg\" alt=\"Knee and joint angles\" style=\"width:100%\">\n  <figcaption>Fig.3 - Knee and joint angles</figcaption>\n</figure>\n\n### Vertical translation of center of mass\n\n<figure>\n  <img src=\"resources/Comparison_Body_V_Pos_new.jpg\" alt=\"Vertical translation of center of mass\" style=\"width:100%\">\n  <figcaption>Fig.4 - Vertical translation of center of mass</figcaption>\n</figure>\n","updated":"2023-11-21T17:48:45.186Z","path":"Publications/Slimdog/index.html","comments":1,"layout":"page","_id":"clp8mraf80003b7k8dex45gu5","content":"<p><strong>Publication</strong> [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28\"><strong>Springer</strong></a>][<a href=\"https://arxiv.org/abs/2101.09961\"><strong>arXiv</strong></a>][<a href=\"https://github.com/keyanzhai/Slimdog\"><strong>code</strong></a>][<a href=\"https://1drv.ms/p/s!AuXcCtfGaQlXg7QnW-4KvFFFTjsgGg?e=eXJ6RU\"><strong>slides</strong></a>]:<br>Keyan Zhai, Chu’an Li, and Andre Rosendo. “Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.” 16th International Conference on Intelligent Autonomous Systems (<a href=\"https://www.ias-16.com/\">IAS-16</a>).</p>\n<p><strong>Abstract</strong><br>During learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure&#x2F;control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.</p>\n<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/failures.mp4\"></video></p>\n\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><figure>\n  <img src=\"resources/Slimdog1.JPG\" alt=\"Slimdog\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Slimdog robot</figcaption>\n</figure>\n\n<figure>\n  <img src=\"resources/Comparison_min_red_no.png\" alt=\"Comparison of no support, minimum support and reducing support\" style=\"width:100%\">\n  <figcaption>Fig.2 - Comparison of no support, minimum support and reducing support</figcaption>\n</figure>\n\n<h2 id=\"Kinovea-Analysis\"><a href=\"#Kinovea-Analysis\" class=\"headerlink\" title=\"Kinovea Analysis\"></a>Kinovea Analysis</h2><h3 id=\"No-support-No-P1-1st-run-fitness-x3D-72-83\"><a href=\"#No-support-No-P1-1st-run-fitness-x3D-72-83\" class=\"headerlink\" title=\"No support (No_P1, 1st run, fitness &#x3D; 72.83)\"></a>No support (No_P1, 1st run, fitness &#x3D; 72.83)</h3><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/No_P1_R1_Kinovea3.mp4\"></video></p>\n\n<h3 id=\"Minimum-support-Min-P1-1st-run-fitness-x3D-72-43\"><a href=\"#Minimum-support-Min-P1-1st-run-fitness-x3D-72-43\" class=\"headerlink\" title=\"Minimum support (Min_P1, 1st run, fitness &#x3D; 72.43)\"></a>Minimum support (Min_P1, 1st run, fitness &#x3D; 72.43)</h3><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Min_P1_R1_Kinovea3.mp4\"></video></p>\n\n<h3 id=\"Reducing-support-Red-P1-3rd-run-fitness-x3D-75-92\"><a href=\"#Reducing-support-Red-P1-3rd-run-fitness-x3D-75-92\" class=\"headerlink\" title=\"Reducing support (Red_P1, 3rd run, fitness &#x3D; 75.92)\"></a>Reducing support (Red_P1, 3rd run, fitness &#x3D; 75.92)</h3><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Red_P1_R3_Kinovea3.mp4\"></video></p>\n\n\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><h3 id=\"Knee-and-joint-angles\"><a href=\"#Knee-and-joint-angles\" class=\"headerlink\" title=\"Knee and joint angles\"></a>Knee and joint angles</h3><figure>\n  <img src=\"resources/Comparison_Joint_Angles.jpg\" alt=\"Knee and joint angles\" style=\"width:100%\">\n  <figcaption>Fig.3 - Knee and joint angles</figcaption>\n</figure>\n\n<h3 id=\"Vertical-translation-of-center-of-mass\"><a href=\"#Vertical-translation-of-center-of-mass\" class=\"headerlink\" title=\"Vertical translation of center of mass\"></a>Vertical translation of center of mass</h3><figure>\n  <img src=\"resources/Comparison_Body_V_Pos_new.jpg\" alt=\"Vertical translation of center of mass\" style=\"width:100%\">\n  <figcaption>Fig.4 - Vertical translation of center of mass</figcaption>\n</figure>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>Publication</strong> [<a href=\"https://link.springer.com/chapter/10.1007/978-3-030-95892-3_28\"><strong>Springer</strong></a>][<a href=\"https://arxiv.org/abs/2101.09961\"><strong>arXiv</strong></a>][<a href=\"https://github.com/keyanzhai/Slimdog\"><strong>code</strong></a>][<a href=\"https://1drv.ms/p/s!AuXcCtfGaQlXg7QnW-4KvFFFTjsgGg?e=eXJ6RU\"><strong>slides</strong></a>]:<br>Keyan Zhai, Chu’an Li, and Andre Rosendo. “Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with Bayesian Optimization.” 16th International Conference on Intelligent Autonomous Systems (<a href=\"https://www.ias-16.com/\">IAS-16</a>).</p>\n<p><strong>Abstract</strong><br>During learning trials, systems are exposed to different failure conditions which may break robotic parts before a safe behavior is discovered. Humans contour this problem by grounding their learning to a safer structure&#x2F;control first and gradually increasing its difficulty. This paper presents the impact of a similar supports in the learning of a stable gait on a quadruped robot. Based on the psychological theory of instructional scaffolding, we provide different support settings to our robot, evaluated with strain gauges, and use Bayesian Optimization to conduct a parametric search towards a stable Raibert controller. We perform several experiments to measure the relation between constant supports and gradually reduced supports during gait learning, and our results show that a gradually reduced support is capable of creating a more stable gait than a support at a fixed height. Although gaps between simulation and reality can lead robots to catastrophic failures, our proposed method combines speed and safety when learning a new behavior.</p>\n<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/failures.mp4\"></video></p>\n\n<h2 id=\"Experiments\"><a href=\"#Experiments\" class=\"headerlink\" title=\"Experiments\"></a>Experiments</h2><figure>\n  <img src=\"resources/Slimdog1.JPG\" alt=\"Slimdog\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Slimdog robot</figcaption>\n</figure>\n\n<figure>\n  <img src=\"resources/Comparison_min_red_no.png\" alt=\"Comparison of no support, minimum support and reducing support\" style=\"width:100%\">\n  <figcaption>Fig.2 - Comparison of no support, minimum support and reducing support</figcaption>\n</figure>\n\n<h2 id=\"Kinovea-Analysis\"><a href=\"#Kinovea-Analysis\" class=\"headerlink\" title=\"Kinovea Analysis\"></a>Kinovea Analysis</h2><h3 id=\"No-support-No-P1-1st-run-fitness-x3D-72-83\"><a href=\"#No-support-No-P1-1st-run-fitness-x3D-72-83\" class=\"headerlink\" title=\"No support (No_P1, 1st run, fitness &#x3D; 72.83)\"></a>No support (No_P1, 1st run, fitness &#x3D; 72.83)</h3><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/No_P1_R1_Kinovea3.mp4\"></video></p>\n\n<h3 id=\"Minimum-support-Min-P1-1st-run-fitness-x3D-72-43\"><a href=\"#Minimum-support-Min-P1-1st-run-fitness-x3D-72-43\" class=\"headerlink\" title=\"Minimum support (Min_P1, 1st run, fitness &#x3D; 72.43)\"></a>Minimum support (Min_P1, 1st run, fitness &#x3D; 72.43)</h3><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Min_P1_R1_Kinovea3.mp4\"></video></p>\n\n<h3 id=\"Reducing-support-Red-P1-3rd-run-fitness-x3D-75-92\"><a href=\"#Reducing-support-Red-P1-3rd-run-fitness-x3D-75-92\" class=\"headerlink\" title=\"Reducing support (Red_P1, 3rd run, fitness &#x3D; 75.92)\"></a>Reducing support (Red_P1, 3rd run, fitness &#x3D; 75.92)</h3><p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/Red_P1_R3_Kinovea3.mp4\"></video></p>\n\n\n<h2 id=\"Results\"><a href=\"#Results\" class=\"headerlink\" title=\"Results\"></a>Results</h2><h3 id=\"Knee-and-joint-angles\"><a href=\"#Knee-and-joint-angles\" class=\"headerlink\" title=\"Knee and joint angles\"></a>Knee and joint angles</h3><figure>\n  <img src=\"resources/Comparison_Joint_Angles.jpg\" alt=\"Knee and joint angles\" style=\"width:100%\">\n  <figcaption>Fig.3 - Knee and joint angles</figcaption>\n</figure>\n\n<h3 id=\"Vertical-translation-of-center-of-mass\"><a href=\"#Vertical-translation-of-center-of-mass\" class=\"headerlink\" title=\"Vertical translation of center of mass\"></a>Vertical translation of center of mass</h3><figure>\n  <img src=\"resources/Comparison_Body_V_Pos_new.jpg\" alt=\"Vertical translation of center of mass\" style=\"width:100%\">\n  <figcaption>Fig.4 - Vertical translation of center of mass</figcaption>\n</figure>\n"},{"title":"Autonomous Elevator Robot","date":"2020-11-17T18:38:49.000Z","academia":true,"_content":"\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/elevator.mp4\"></video></p>\n\n## Introduction\n\nIn this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.\n\n## System Description\n\n### Hardware\n\nOur elevator robot includes a complex hardware system consists of the following parts:\n\n* **Base platform** - Jackal\n* **Manipulator** - Kinova\n* **Camera** - Intel Realsense\n* **3D LiDAR** - Velodyne\n\n<figure>\n  <img src=\"resources/Elevator_Robot.JPG\" alt=\"The Elevator Robot\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Elevator Robot</figcaption>\n</figure>\n\n### Software\n\nThe whole software system is a multi-phased pipeline based on ROS melodic on Ubuntu 18.04.\n\n* **Localization and Navigation** - AMCL algorithm\n\n<figure>\n  <img src=\"resources/Navi.png\" alt=\"AMCL Navigation\" style=\"width:100%\">\n  <figcaption>Fig.2 - AMCL Navigation</figcaption>\n</figure>\n\n* **Button Detection** - OCR-RCNN algorithm\n\n<figure>\n  <img src=\"resources/Button_Detect.JPG\" alt=\"Button Detection\" style=\"width:100%\">\n  <figcaption>Fig.3 - AMCL Navigation</figcaption>\n</figure>\n\n* **Button Push** - MoveIt!\n\n<figure>\n  <img src=\"resources/Button_Push.jpg\" alt=\"Button Push\" style=\"width:100%\">\n  <figcaption>Fig.4 - Button Push</figcaption>\n</figure>\n\nThe upper left image indicates a random starting pose of the manipulator, represented by the orange arm in RViz in the right.\n\nThe lower left image indicates the target pose of the manipulator where it pushes the button with the pose represented by the red arrow in RViz in the right.\n","source":"Projects/Elevator/index.md","raw":"---\ntitle: Autonomous Elevator Robot\ndate: 2020-11-17 13:38:49\nacademia: true\n---\n\n<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/elevator.mp4\"></video></p>\n\n## Introduction\n\nIn this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.\n\n## System Description\n\n### Hardware\n\nOur elevator robot includes a complex hardware system consists of the following parts:\n\n* **Base platform** - Jackal\n* **Manipulator** - Kinova\n* **Camera** - Intel Realsense\n* **3D LiDAR** - Velodyne\n\n<figure>\n  <img src=\"resources/Elevator_Robot.JPG\" alt=\"The Elevator Robot\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Elevator Robot</figcaption>\n</figure>\n\n### Software\n\nThe whole software system is a multi-phased pipeline based on ROS melodic on Ubuntu 18.04.\n\n* **Localization and Navigation** - AMCL algorithm\n\n<figure>\n  <img src=\"resources/Navi.png\" alt=\"AMCL Navigation\" style=\"width:100%\">\n  <figcaption>Fig.2 - AMCL Navigation</figcaption>\n</figure>\n\n* **Button Detection** - OCR-RCNN algorithm\n\n<figure>\n  <img src=\"resources/Button_Detect.JPG\" alt=\"Button Detection\" style=\"width:100%\">\n  <figcaption>Fig.3 - AMCL Navigation</figcaption>\n</figure>\n\n* **Button Push** - MoveIt!\n\n<figure>\n  <img src=\"resources/Button_Push.jpg\" alt=\"Button Push\" style=\"width:100%\">\n  <figcaption>Fig.4 - Button Push</figcaption>\n</figure>\n\nThe upper left image indicates a random starting pose of the manipulator, represented by the orange arm in RViz in the right.\n\nThe lower left image indicates the target pose of the manipulator where it pushes the button with the pose represented by the red arrow in RViz in the right.\n","updated":"2023-11-21T17:48:45.123Z","path":"Projects/Elevator/index.html","comments":1,"layout":"page","_id":"clp8mraf90004b7k8gqmg1eyv","content":"<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/elevator.mp4\"></video></p>\n\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>In this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.</p>\n<h2 id=\"System-Description\"><a href=\"#System-Description\" class=\"headerlink\" title=\"System Description\"></a>System Description</h2><h3 id=\"Hardware\"><a href=\"#Hardware\" class=\"headerlink\" title=\"Hardware\"></a>Hardware</h3><p>Our elevator robot includes a complex hardware system consists of the following parts:</p>\n<ul>\n<li><strong>Base platform</strong> - Jackal</li>\n<li><strong>Manipulator</strong> - Kinova</li>\n<li><strong>Camera</strong> - Intel Realsense</li>\n<li><strong>3D LiDAR</strong> - Velodyne</li>\n</ul>\n<figure>\n  <img src=\"resources/Elevator_Robot.JPG\" alt=\"The Elevator Robot\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Elevator Robot</figcaption>\n</figure>\n\n<h3 id=\"Software\"><a href=\"#Software\" class=\"headerlink\" title=\"Software\"></a>Software</h3><p>The whole software system is a multi-phased pipeline based on ROS melodic on Ubuntu 18.04.</p>\n<ul>\n<li><strong>Localization and Navigation</strong> - AMCL algorithm</li>\n</ul>\n<figure>\n  <img src=\"resources/Navi.png\" alt=\"AMCL Navigation\" style=\"width:100%\">\n  <figcaption>Fig.2 - AMCL Navigation</figcaption>\n</figure>\n\n<ul>\n<li><strong>Button Detection</strong> - OCR-RCNN algorithm</li>\n</ul>\n<figure>\n  <img src=\"resources/Button_Detect.JPG\" alt=\"Button Detection\" style=\"width:100%\">\n  <figcaption>Fig.3 - AMCL Navigation</figcaption>\n</figure>\n\n<ul>\n<li><strong>Button Push</strong> - MoveIt!</li>\n</ul>\n<figure>\n  <img src=\"resources/Button_Push.jpg\" alt=\"Button Push\" style=\"width:100%\">\n  <figcaption>Fig.4 - Button Push</figcaption>\n</figure>\n\n<p>The upper left image indicates a random starting pose of the manipulator, represented by the orange arm in RViz in the right.</p>\n<p>The lower left image indicates the target pose of the manipulator where it pushes the button with the pose represented by the red arrow in RViz in the right.</p>\n","site":{"data":{}},"excerpt":"","more":"<p align=\"center\"><video style=\"width:100%\" controls>\n<source src=\"resources/elevator.mp4\"></video></p>\n\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>In this project, we tried to make a robot capable of taking elevators autonomously. With the ability to take elevators and move vertically inside a building, the mobility of wheeled robots can be significantly improved.</p>\n<h2 id=\"System-Description\"><a href=\"#System-Description\" class=\"headerlink\" title=\"System Description\"></a>System Description</h2><h3 id=\"Hardware\"><a href=\"#Hardware\" class=\"headerlink\" title=\"Hardware\"></a>Hardware</h3><p>Our elevator robot includes a complex hardware system consists of the following parts:</p>\n<ul>\n<li><strong>Base platform</strong> - Jackal</li>\n<li><strong>Manipulator</strong> - Kinova</li>\n<li><strong>Camera</strong> - Intel Realsense</li>\n<li><strong>3D LiDAR</strong> - Velodyne</li>\n</ul>\n<figure>\n  <img src=\"resources/Elevator_Robot.JPG\" alt=\"The Elevator Robot\" style=\"width:100%\">\n  <figcaption>Fig.1 - The Elevator Robot</figcaption>\n</figure>\n\n<h3 id=\"Software\"><a href=\"#Software\" class=\"headerlink\" title=\"Software\"></a>Software</h3><p>The whole software system is a multi-phased pipeline based on ROS melodic on Ubuntu 18.04.</p>\n<ul>\n<li><strong>Localization and Navigation</strong> - AMCL algorithm</li>\n</ul>\n<figure>\n  <img src=\"resources/Navi.png\" alt=\"AMCL Navigation\" style=\"width:100%\">\n  <figcaption>Fig.2 - AMCL Navigation</figcaption>\n</figure>\n\n<ul>\n<li><strong>Button Detection</strong> - OCR-RCNN algorithm</li>\n</ul>\n<figure>\n  <img src=\"resources/Button_Detect.JPG\" alt=\"Button Detection\" style=\"width:100%\">\n  <figcaption>Fig.3 - AMCL Navigation</figcaption>\n</figure>\n\n<ul>\n<li><strong>Button Push</strong> - MoveIt!</li>\n</ul>\n<figure>\n  <img src=\"resources/Button_Push.jpg\" alt=\"Button Push\" style=\"width:100%\">\n  <figcaption>Fig.4 - Button Push</figcaption>\n</figure>\n\n<p>The upper left image indicates a random starting pose of the manipulator, represented by the orange arm in RViz in the right.</p>\n<p>The lower left image indicates the target pose of the manipulator where it pushes the button with the pose represented by the red arrow in RViz in the right.</p>\n"}],"Post":[{"title":"Introduction","date":"2023-07-02T05:04:42.000Z","academia":true,"_content":"\n# About me\n\nMy name is Keyan Zhai, or 翟珂岩 in Chinese. You can call me KZ for short.\n\nI got my Master's degree in Robotics from University of Pennsylvania in May 2023. \n\n# Research\n\nMy research interests lie at the intersection of Computer Vision, Computer Graphics and Robotics, with a focus on the perception, representation, and understanding of the 3D world. \n\nI am particularly excited about the following topics:\n\n- **Perception**: state estimation, sensor fusion, visual odometry, visual SLAM, structure from motion, egocentric vision\n- **Representation**: 3D/4D reconstruction, neural scene representation, neural rendering, image synthesis\n- **Understanding**: 3D scene understanding, point cloud analysis, instance/semantic segmentation, pose estimation\n\nMy ultimate aim is to digitize humans, objects and events in the physical world and explore their applications in industries like AR/VR, autonomous driving and Robotics.","source":"_posts/Introduction.md","raw":"---\ntitle: Introduction \ndate: 2023-07-02 01:04:42\ntags:\nacademia: true\n---\n\n# About me\n\nMy name is Keyan Zhai, or 翟珂岩 in Chinese. You can call me KZ for short.\n\nI got my Master's degree in Robotics from University of Pennsylvania in May 2023. \n\n# Research\n\nMy research interests lie at the intersection of Computer Vision, Computer Graphics and Robotics, with a focus on the perception, representation, and understanding of the 3D world. \n\nI am particularly excited about the following topics:\n\n- **Perception**: state estimation, sensor fusion, visual odometry, visual SLAM, structure from motion, egocentric vision\n- **Representation**: 3D/4D reconstruction, neural scene representation, neural rendering, image synthesis\n- **Understanding**: 3D scene understanding, point cloud analysis, instance/semantic segmentation, pose estimation\n\nMy ultimate aim is to digitize humans, objects and events in the physical world and explore their applications in industries like AR/VR, autonomous driving and Robotics.","slug":"Introduction","published":1,"updated":"2023-11-21T17:48:45.325Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clp8mraf60001b7k874fv9n3t","content":"<h1 id=\"About-me\"><a href=\"#About-me\" class=\"headerlink\" title=\"About me\"></a>About me</h1><p>My name is Keyan Zhai, or 翟珂岩 in Chinese. You can call me KZ for short.</p>\n<p>I got my Master’s degree in Robotics from University of Pennsylvania in May 2023. </p>\n<h1 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h1><p>My research interests lie at the intersection of Computer Vision, Computer Graphics and Robotics, with a focus on the perception, representation, and understanding of the 3D world. </p>\n<p>I am particularly excited about the following topics:</p>\n<ul>\n<li><strong>Perception</strong>: state estimation, sensor fusion, visual odometry, visual SLAM, structure from motion, egocentric vision</li>\n<li><strong>Representation</strong>: 3D&#x2F;4D reconstruction, neural scene representation, neural rendering, image synthesis</li>\n<li><strong>Understanding</strong>: 3D scene understanding, point cloud analysis, instance&#x2F;semantic segmentation, pose estimation</li>\n</ul>\n<p>My ultimate aim is to digitize humans, objects and events in the physical world and explore their applications in industries like AR&#x2F;VR, autonomous driving and Robotics.</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"About-me\"><a href=\"#About-me\" class=\"headerlink\" title=\"About me\"></a>About me</h1><p>My name is Keyan Zhai, or 翟珂岩 in Chinese. You can call me KZ for short.</p>\n<p>I got my Master’s degree in Robotics from University of Pennsylvania in May 2023. </p>\n<h1 id=\"Research\"><a href=\"#Research\" class=\"headerlink\" title=\"Research\"></a>Research</h1><p>My research interests lie at the intersection of Computer Vision, Computer Graphics and Robotics, with a focus on the perception, representation, and understanding of the 3D world. </p>\n<p>I am particularly excited about the following topics:</p>\n<ul>\n<li><strong>Perception</strong>: state estimation, sensor fusion, visual odometry, visual SLAM, structure from motion, egocentric vision</li>\n<li><strong>Representation</strong>: 3D&#x2F;4D reconstruction, neural scene representation, neural rendering, image synthesis</li>\n<li><strong>Understanding</strong>: 3D scene understanding, point cloud analysis, instance&#x2F;semantic segmentation, pose estimation</li>\n</ul>\n<p>My ultimate aim is to digitize humans, objects and events in the physical world and explore their applications in industries like AR&#x2F;VR, autonomous driving and Robotics.</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}